% !TEX root = z_output/_ProbMeth.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% 80 characters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\input{../macros.tex}
\usepackage{makeidx}

\def\excludeversion#1{\includeversion{#1}}

\excludeversion{prelims}
\excludeversion{chapter1}
\excludeversion{chapter2}
\excludeversion{chapter3}
\includeversion{chapter4}
\excludeversion{chapter5}
\excludeversion{chapter6}
\includeversion{chapter7}

%not yet:



\includeversion{chapter9}



\usepackage{multicol}
\makeatletter
\renewenvironment{theindex}
  {\if@twocolumn
      \@restonecolfalse
   \else
      \@restonecoltrue
   \fi
   \setlength{\columnseprule}{0pt}
   \setlength{\columnsep}{10pt}% I CHANGED THIS from 35pt
   \begin{multicols}{3}[\section*{\indexname}]
   \markboth{\MakeUppercase\indexname}%
            {\MakeUppercase\indexname}%
   \thispagestyle{plain}
   \setlength{\parindent}{0pt}
   \setlength{\parskip}{0pt plus 0.3pt}
   \relax
   \let\item\@idxitem
   \small}% I ADDED THIS
  {\end{multicols}\if@restonecol\onecolumn\else\clearpage\fi}
\makeatother


\makeindex
\newcommand{\indexThmCode}{}
\newcommand{\Index}[1]{\index{#1}#1}
\newcommand{\indexThm}[1]{\index{Hartshorne!Theorems, etc.!#1}}
\newcommand{\Entry}[3][notcustom]{\Bullet
\ifthenelse{\equal{#1}{}}{}
{
\ifthenelse{\equal{#1}{notcustom}}{\index{#2}}{\index{#1}}
}
 \textbf{#2:}&#3}

%\newenvironment{INT}{\begin{itemize}\small\item}{\end{itemize}}

\newenvironment{INT}[1][]{\begin{itemize}\small\item\textbf{#1}}{\end{itemize}}
\newcommand{\moreINT}[1][]{\item\textbf{#1}}



\begin{document}
\begin{prelims}
\section*{\S$-\infty$: Preliminaries}
\begin{itemise}
\item Asymptotic notations
\begin{itemize}\squishlist
\item \makebox[4.5em][l]{$f= O(g)$} means ``$f\leq cg$''.
\item \makebox[4.5em][l]{$f=o(g)$} means ``$f/g\rightarrow0$''.
\item \makebox[4.5em][l]{$f=\Omega(g)$} means ``$f\geq cg$''.
\item \makebox[4.5em][l]{$f=\Theta(g)$} means ``$c_1 g\leq f\leq c_2 g$''.
\item \makebox[4.5em][l]{$f\sim g$} means ``$f/g\rightarrow1$''.
\end{itemize}
\item $1-p\leq e^{-p}$ for all nonnegative $p$, close for small $p$.
\item The Stirling approximation: $k!\sim \sqrt{2\pi k}(k/e)^k$.
\item Some bounds on ${n\choose k}$:
\[\left(\frac{n}{k}\right)^k\leq {n\choose k}\leq\frac{n^k}{k!}\leq\left(\frac{n\cdot e}{k}\right)^k.\]
\item On central binomial coefficients:
\[\frac{2^{2n-1}}{\sqrt n}\leq {2n\choose n}\sim\frac{4^n}{\sqrt{\pi n}}.\]
\item Let $S_n$ be the sum of $n$ iid uniform $\{\pm1\}$ random variables. Then:
\[\Exp[|S_n|]=n2^{1-n}{n-1\choose\lfloor(n-1)/2\rfloor}=(\sqrt{2/\pi}+o(1))\sqrt n.\]
\end{itemise}
\end{prelims}
\begin{chapter1}
\section*{\S1: The basic method}
\begin{itemise}
\item The \textbf{Ramsey number} $R(k,l)$ is the least $n$ such that for any 2-colouring of the vertices of the complete graph $K_n$ on $n$ vertices, there is always either a blue $K_k$ or a red $K_l$.
\begin{INT}[Prop 1.1.1:]
If ${n\choose k}2^{1-{k\choose 2}}<1$ then $R(k,k)>n$.
\begin{proof}
Want to show that there's a chance you fail to get a monochromatic subgraph from a random colouring. Add up the probabilities of getting a monochromatic subgraph at each opportunity, to get ${n\choose k}2^{1-{k\choose 2}}$. It is possible that none of these occur.
\end{proof}
\end{INT}
\item A \textbf{tournament} on a set $V$ of $n$ players is an orientation of the complete graph $K_V$. That is, every pair of players has a unique winner.
A tournament could have \textbf{property $S_k$}: for every group of $k$ players, there is one that beats them all.
\begin{INT}[Thm 1.2.1:]
If ${n\choose k}(1-2^{-k})^{n-k}<1$ then there is a tournament with $S_k$ of size $n$.
\begin{proof}
Choose a random tournament. The chance that it fails to have $S_k$ is bounded above by this probability.
\end{proof}
\end{INT}
\item A \textbf{dominating set} in a graph $(V,E)$ is a set $U$ of vertices to which every vertex $v\in V-U$ is connected by an edge.
\begin{INT}[Thm 1.2.2:]
A graph on $n$ vertices with minimum degree $\delta>1$ has a dominating set of at most $n[1+\log(\delta+1)]/(\delta+1)$ vertices.
\begin{proof}
Fix a probability $p$. Choose vertices of with probability $p$. The expected size of the set $\{\text{vertices chosen}\}\cup\{\text{vertices not neighbouring the chosen set}\}$ is at most $pn+(1-p)^{\delta+1}n$, by linearity of expectation (also note: alteration). Thus there is a dominating set of size at most this amount. Using $1-p\leq e^{-p}$ to simplify, and then minimizing, we get the result.
\end{proof}
\end{INT}
\item A \textbf{cut} in a graph is simply a partition of the edges into two nonempty disjoint subsets. The \textbf{size} of a cut is the number of edges whose endpoints lie in opposite sides. The \textbf{edge connectivity} is the minimum size of a cut.
\item A \textbf{hypergraph} $(V,E)$ is a finite set $V$ of vertices, and a subset $E\subset 2^V$. It is \textbf{$n$-uniform} if each edge contains $n$ vertices. It has \textbf{property B}, or is \textbf{two-colourable}, if there is a two-colouring of the vertices so that no edge is monochromatic. Let $m(n)$ be the minimum number of edges of an $n$-uniform hypergraph which is not two-colourable.
\begin{INT}[Prop 1.3.1:]
Every $n$-uniform hypergraph with less that $2^{n-1}$ edges is two-colourable, so that $m(n)\geq 2^{n-1}$.
\begin{proof}
Randomly colour the vertices. The probability of failing to give a valid 2-colouring is less than $2^{n-1}\cdot 2^{1-n}$.
\end{proof}
\item \textbf{Thm 1.3.2:} $m(n)<(1+o(1))\frac{e\log2}{4}n^22^n$.
\begin{proof}
We need to construct a small $n$-uniform hypergraph which is not 2-colourable. Let $v$ and $m$ be to be determined. Then choose a set $V$ of size $v$ for the vertices. A random subset $S$ of size $n$ has probability $({a\choose n}+{b\choose n})/{v\choose n}$ of being monochromatic (in a colouring with $a$ and $b$ of each colour), which is at least $p=2{v/2\choose n}/{v\choose n}$, by convexity (if $v$ is even).

\INDENT Now choose $m$ random sets of size $n$ independently. The probability that a given colouring is a 2-colouring is at most $(1-p)^m$, so the probability that there exists a 2-colouring is at most $2^v(1-p)^m$. Thus, if this quantity is less than 1, we have $m(n)\leq m$. Now it's asymptotics.
\end{proof}
\end{INT}
\item A \textbf{$(k,l)$-system} is a family $\calF=\{(A_i,B_i)\}_{i=1}^h$ pairs of sets\footnote{Really, pairs of subsets of an arbitrary set.}, such that $|A_i|=k$, $|B_i|=l$, and $A_i\cap B_j$ is empty \Iff $i=j$, for $1\leq i,j\leq h$.
\begin{INT}[Thm 1.3.3:]
If $\calF=\{(A_i,B_i)\}_{i=1}^h$ is a $(k,l)$-system, then $h\leq {k+l\choose k}$.
\begin{proof}
Let $X=\bigcup_i (A_i\cup B_i)$, and randomly order $X$. Let $X_i$ be the event that the elements of $A_i$ precede those of $B_i$. If these events are pairwise disjoint, we have $1\geq \sum \PP[X_i]=h/{k+l\choose k}$.

\INDENT Suppose then that both $X_i$ and $X_j$ can occur simultaneously. Then there's an ordering such that $A_i$ precedes $B_i$ and $A_j$ precedes $B_j$. WLOG the last element of $A_i$ precedes that of $A_j$, but then $A_i$ precedes $B_j$, contradicting that $A_i\cap B_j$ is nonempty.
\end{proof}
\item This is sharp, writing $\calF=\{(A,X\setminus A) : A\subset X, |A|=k\}$, where $|X|=k+l$.
\end{INT}
\item A subset $A$ of an abelian group is \textbf{sum-free} if $(A+A)\cap A$ is empty.
\begin{INT}[Thm 1.4.1:]
Every set $A$ of $n$ nonzero integers contains a sum-free subset of size at least $n/3$.
\begin{proof}
Choose a prime $p=3k+2$ far larger than the largest absolute value of any element of $A$. Embed the question in the field $\F=\Z/p\Z$, where it is unchanged. In this field, there is a large sum-free subset $C=\{k+1,\ldots,2k+1\}$, which has at least $1/3$ of the elements. Choose a random $x\in\F^\times$, and consider $xA\cap C$. The average size of this set is at least $|A|/3$, and it is always sum-free!
\end{proof}
\end{INT}
\item For $\calF$ a family of subsets of $\{1,\ldots,n\}$, let $d(\calF)$ be the number of unordered \textbf{pairs of disjoint elements} of $\calF$.
\begin{INT}[Thm 1.5.1:]
Let $\calF$ be a family of $m=2^{(1/2+\delta)n}$ subsets of $\{1,\ldots,n\}$. Then $d(\calF)<m^{2-\delta^2/2}$.
\begin{proof}
Suppose for contradiction that $d(\calF)\geq m^{2-\delta^2/2}$. Choose an integer $t$, to be determined. Choose $A_1,\ldots,A_t$ independently and with repetition from $\calF$. The point is that it will be likely that $U=\bigcup A_i$ has at least $n/2$ elements, and that there are at least $2^{n/2}$ distinct subsets of $X$ disjoint from $U$, a contradiction. For the first, it's really unlikely that $U$ will be contained in any given subset $S$ of size $n/2$, so:
\[\Prob[|U|\leq n/2]\leq \sum_{|S|=n/2}\Prob[\text{all $A_i\subset S$}]\leq 2^n\left(\frac{2^{n/2}}{m}\right)^t=2^{n(1-\delta t)}.\]
Note: One has that $A_i$ could be any of $m$ options, and at most $2^{n/2}$ of these lie in each $S$. We used a very clumsy approximation $2^n\geq |\{|S|=n/2\}|$.

\INDENT Now to the other side. We have a function $v:\calF\to\N$ which maps $B$ to the number of disjoint pairs its a member of. Then $v$ takes average value $2d(\calF)/m\geq2m^{1-\delta^2/2}$. Let $Y$ be the r.v.\ returning the number of $B\in \calF$ disjoint to $U$. Then we want to see that $Y$ often exceeds $2^{n/2}$. Applying convexity:
\[E[Y]=
%\sum\left(\frac{v(b)}{m}\right)^t=
m^{1-t}\left(\frac{\sum v(b)^t}{m}\right)\geq m^{1-t}\left(2m^{1-\delta^2/2}\right)^t=2^tm^{1-t\delta^2/2}=2^{t+(1/2+\delta)n(1-t\delta^2/2)}.\]
We'd like to bound $p=\Prob[Y\geq 2^n/2]$. As $Y<m$, we've got an inequality
\[pm+(1-p)2^{n/2}\geq E[Y].\]
This shows that $p$ is pretty big, and in fact, for some good $t$, bigger than $2^{n(1-\delta t)}$.
\end{proof}
\end{INT}
\end{itemise}
\subsection*{The Probabilistic Lens: The Erd\H{o}s-Ko-Rado theorem}
A family $\calF$ of sets is \textbf{intersecting} if any $A,B\in\calF$ satisfy $A\cap B\neq\emptyset$. We are interested in large intersecting families of $k$-element subsets of $\{0,\ldots, n-1\}$ (where $k\leq n/2$). One option is the set of $k$-element subsets containing a given element.
\begin{INT}[Erd\H{o}s-Ko-Rado:]
This is best possible --- any intersecting family of $k$-element subsets of an $n$-element set has size at most ${n-1\choose k-1}$.
\begin{proof}
Suppose $\calF$ is such. View $\{0,\ldots, n-1\}$ as $\Z/n\Z$. Let $A_0=\{0,\ldots,k-1\}$ and define translates $A_s=A_0+s$. Then $\calF$ contains at most $k$ of the $A_s$. [Suppose $A_0\in\calF$, the other $A_t$ intersecting can be arranged in $k-1$ pairs which are disjoint.]

Now for any $k$-element subset $A$ of $\Z/n\Z$ and a random $i\in \Z/n\Z$, the probability that $A+i\in\calF$ is at most $k/n$. Thus, if we randomise $A$ uniformly amongst the $k$-sets, the probability that $A+i\in\calF$ is still at most $k/n$. Now $A+i$ is uniformly chosen from amongst the $k$-sets, so that the probability a random $k$-element subset is in $\calF$ is at most $k/n$, giving the result.
\end{proof}
\end{INT}
%  Suppose $\calF$ is an intersecting family of 
\end{chapter1}

\begin{chapter2}
\section*{\S2: Linearity of expectation}
\begin{itemise}
\item A \textbf{Hamiltonian path} is a path in a graph which visits each vertex exactly once. A \textbf{Hamiltonian cycle} is such which returns to its starting vertex.
\begin{INT}[Thm 2.1.1:]
There's a tournament $T$ with $n$ players and at least $n!2^{-(n-1)}$ Hamiltonian paths.
\begin{proof}
For this is the expected number of Hamiltonian paths in a random Tournament.
\end{proof}
\end{INT}
\item A \textbf{bipartite graph} is one in which the vertices can be partitioned into two disjoint subsets, such that the induced subgraph on each has no edges.
\begin{INT}[Thm 2.2.1:]
Any graph with $e$ edges contains a bipartite subgraph (not necessarily complete) with at least $e/2$ edges.
\begin{proof}
For each vertex, randomly assign it to either $A$ or $B$, to give a random partition. On average, there should be $e/2$ crossing edges.
\end{proof}
\moreINT[Thm 2.2.2:]
If $G$ has $2n$ vertices and $e$ edges, then it contains a bipartite subgraph with at least $en/(2n-1)$ edges. [Similarly, get $e(n+1)/(2n+1)$ edges given $2n+1$ vertices.]
\begin{proof}
Choose $A$ uniformly from all $n$-element subsets of $V$. Same deal.
\end{proof}
\end{INT}
\item \rednote{There's something nasty which I don't get about 2.2.3.}
\item Let $K_{a,b}$ be the graph $(A\sqcup B,\text{``$A\times B$''})$ where $|A|=a$ and $|B|=b$.
\begin{INT}[Thm 2.3.1:]
There's a 2-colouring of $K_n$ with at most ${n\choose a}2^{1-{a\choose 2}}$ monochromatic $K_a$.
\begin{proof}
This is the expected number of monochromatic $K_a$ in a random 2-colouring.
\end{proof}
\moreINT[Thm 2.3.2:] Similarly, there's a 2-colouring of $K_n$ with at most ${m\choose a}{n\choose b}2^{1-ab}$ monochromatic $K_{a,b}$.
\end{INT}
\item \textbf{Balancing vectors:}
\begin{INT}[Thm 2.4.1:]
Suppose that $v_1,\ldots,v_n\in\R^n$ all have length one. Then there is a signed sum $\pm\epsilon_1 v_1\pm\epsilon_2v_2\pm\cdots\pm\epsilon_n v_n$ of length at least (and one with length at most) $\sqrt n$.
\begin{proof}
For one can calculate that the average norm-squared random signed sum is $n$.
\end{proof}
\moreINT[Thm 2.4.2:]
Let $v_1,\ldots,v_n\in\R^n$ have length at most one. Fix weights $p_1,\ldots,p_n\in[0,1]$, and let $w=\sum p_iv_i$ be the weighted sum. Then there exists $S\subset \{1,\ldots,n\}$ with $|w-\sum_{i\in S} v_i|\leq\sqrt n/2$.
\begin{proof}
Let $\epsilon_i$ be one with probability $p_i$, else zero. Let $v=\sum \epsilon_iv_i$, and let $X=|w-v|^2$. The expectation of $X$ is calculated just like the variance of $\sum\epsilon_i$, with a bunch of inner products multiplied in, but all these inner products are less than one in magnitude, so $\Exp[X]\leq n/4$.
\end{proof}
\end{INT}
\item \textbf{Unbalancing lights:} Given an $n\times n$ array of lights, some on and some off, and $2n$ switches, one for each row and column, that toggle each light therein. Then it's possible to flick switches in order to ensure that there are $(\sqrt{2/\pi}+o(1))n^{3/2}$ more lights on than off.
\begin{INT}
%Let $a_{ij}=\pm1$ depending whether the corresponding light is on or off. We want to maximise $\sum_{ij}a_{ij}x_iy_j$ for $x_i,y_j$ all $\pm1$, and claim that this quantity can be at least $(\sqrt{2/\pi}+o(1))n^{3/2}$.
\begin{proof}
Randomly flick the switches for each column. Let $R_i$ be the difference of the number on and off in the $i^\text{th}$ row. This is has distribution $S_n$ --- the sum of $n$ iid uniform $\{\pm1\}$ random variables. Now $\Exp[|S_n|]=(\sqrt{2/\pi}+o(1))\sqrt n$. Thus $\Exp[\sum|R_i|]$ is the quantity we desire.
\end{proof}
\end{INT}
\end{itemise}
\subsection*{The Probabilistic Lens: Br\'egman's theorem}
The \textbf{permanent} of a matrix is the same sum as the determinant, taken without the signs. We want to bound the permanent of an $n\times n$ $\{0,1\}$-matrix $A$ with $r_i$ ones in the $i^\text{th}$ row. Note that one might be interested in doing this as the permanent of the matrix is the number of cycle coverings in the digraph associated with $A$.
\begin{INT}[Br\'egman's theorem:]
$\text{per}(A)\leq \prod (r_i!)^{1/r_i}$.
\begin{proof}
To solve this problem, we use the \textbf{geometric mean}. That is, if $Y$ is a random variable, we define $\GeomMean(Y):=\exp(\Exp(\log Y))$. Note that $\GeomMean(XY)=\GeomMean(X)\GeomMean(Y)$ even when $X$ and $Y$ are not independent. Note that if $Y$ takes values $a_i$ with probabilities $p_i$, we have $\GeomMean(Y)=\prod a_i^{p_i}$. Just as $\Exp(X)$ is the mean of the conditional expectations, $\GeomMean(Y)$ is the geometric mean of the conditional geometric means.

\INDENT Define an ordered transversal of $A$ to be a pair $(\alpha,\beta)\in\Sigma_n\times\Sigma_n$ of permutations such that $A_{\alpha(i),\beta(i)}=1$ for all $i$. That is, a sequence of ones contributing one to the permanent, taken with an order. Define a transversal to be a permutation $\gamma$ such that $A_{i,\gamma(i)}=1$. Let $\calO$ be the set of ordered transversals, and let $\calT$ be the set of transversals. There's a  function $u:\calO\to\calT$, forgetting the order, whose fibres each have order $n!$. There's a projection $\pi_1:\calO\to \Sigma_n$.


\INDENT Choose $o=(\alpha,\beta)$ randomly from $\calO$. For $i=1,\ldots, n$, remove the $\alpha(i)^\text{th}$ row and the $\beta(i)^\text{th}$ column (containing the $i^\text{th}$ one in the ordering), and let $R_i$ be the number of ones remaining in the $\alpha(i)^\text{th}$ row just before it is removed. Define $L=\prod R_i$. The random variable $L$ should overestimate $\text{per}(A)$, in the sense that $\text{per}(A)\leq\GeomMean(L)$. We'll come back and prove this. We calculate:
\begin{alignat*}{2}
\GeomMean[L|u(o)=\sigma]&=\prod\GeomMean[R_i|u(o)=\sigma]%&\qquad&\text{()}
\\&=\prod(r_i)^{1/r_i}(r_i-1)^{1/r_i}\cdots(1)^{1/r_i}&\qquad&\text{($R_i$ is uniform on $\{1,\ldots, r_i\}$)}
\\&=\prod(r_i!)^{1/r_i}&\qquad&\text{which is independent of $\sigma$, so}
\\\GeomMean[L]&=\prod(r_i!)^{1/r_i}&\qquad&\text{(each $\sigma$ is equally likely)}
\end{alignat*}
\INDENT Now we now should show that $\text{per}(A)\leq\GeomMean(L)$	. We show this conditionally on $\pi_1(o)=\tau$ for any $\tau\in\Sigma_n$. That is, we fix the order of deletion of the rows, and randomise the order of deletion of the columns. Let $r=r_{\tau(1)}$ nonzero entries in the $\tau(1)^\text{th}$ row, in the $e_1,\ldots,e_r$ columns. Let $t_i$ be the perm-cofactor opposite the $i^\text{th}$ nonzero entry. The probability that we first delete the $(e_i)^\text{th}$ column first is $\Prob[\beta(i)=e_i|\pi_1(o)=\tau]=t_i/\text{per}(A)$. Thus (letting $t=\sum t_i/r=\text{per}(A)/r$):
\begin{alignat*}{3}
\qquad\GeomMean[L|\pi_1(o)=\tau]&=\prod_{i=1}^r\GeomMean[rR_2\cdots R_n|\pi_1(o)=\tau,\ \beta(1)=e_i]^{t_i/\text{per}(A)}%&\qquad&\text{()}
\\&\geq r\prod_{i=1}^r(t_i)^{t_i/\text{per}(A)} &\qquad&\text{(by induction)}
\\&=r\left(\prod_{i=1}^r(t_i)^{t_i}\right)^{1/\text{per}(A)}%&\qquad&\text{()}
\\&\geq r\left(\left(t^t\right)^r\right)^{1/\text{per}(A)}&\qquad&\text{(convexity)}
\\&=rt=\text{per}(A)&&&\qquad\qquad\qedhere%&\qquad&\text{()}
\end{alignat*}
\end{proof}
\end{INT}
\end{chapter2}
\begin{chapter3}
\section*{\S3: Alterations}
\begin{itemise}
\item Recall that $R(k,l)>n$ \Iff there is a two-colouring of the edges of $K_n$ with neither a red $K_k$ nor blue $K_l$.
\begin{INT}[Thm 3.1.1:]
For any integer $n$, $R(k,k)>n-{n\choose k}2^{1-{k\choose2}}$.
\begin{proof}
For choosing a random two-colouring of $K_n$, ${n\choose k}2^{1-{k\choose2}}$ is the average number of monochromatic $K_k$. There's some colouring with at most this many; remove a vertex from each.
\end{proof}
There are a couple of analogous results on off-diagonal Ramsey numbers.
\end{INT}
\item The \textbf{independence number} $\alpha(G)$ of a graph $G$ is the maximal size of a collection of vertices of $G$ sharing no edges.
\begin{INT}[Thm 3.2.1:]
Let $G$ have $n$ vertices and $nd/2$ edges ($d\geq1$). Then $\alpha(G)\geq n/(2d)$.
\moreINT[Restatement:] In $G$ has $v$ vertices and $e$ edges with $e\geq n/2$, then $\alpha(G)\geq (n/2)^2/e$.
\begin{proof}
Assign each vertex independently a probability of $p$ on being put in a set $S$. Then the average number of elements of $S$ is $pn$, and the average number of edges in the induced subgraph is $p^2e$. We thus obtain a randomised method of returning an independent subset of $G$ with an average of $pn-p^2e$ elements. This quantity is maximised at $p=n/(2e)\in[0,1]$.
\end{proof}
\end{INT}
\item \textbf{Combinatorial geometry:} We are interested in finding ways to distribute $n$ points in a unit square such that all of the triangles that can be formed from the $n$ points are large in area. For $S\subset I^2$, let $T(S)$ be the area of the smallest triangle with corners in $S$, and let $T(n)$ be the largest value of $T(S)$ as $S$ ranges over subsets of $I^2$ of size $n$.

\begin{INT}[Thm 3.3.1:]
There is a set of $n$ points is the unit square such that $T(S)\geq1/(10n)^2$, so that $T(n)=\Omega(n^{-2})$.
\begin{proof}
For any $\epsilon>0$, suppose that $P,Q,R$ are chosen uniformly from $I^2$. Then it is an easy exercise to bound $\Prob[\text{Area}(PQR)\leq\epsilon]\leq16\pi\epsilon$.

\INDENT Now choose at random $2n$ points in $I^2$. Then expected number of triangles with area less that $1/(10n)^2$ is at most ${2n\choose3}\cdot16\pi/(10n)^2<n$. There is a configuration with less than $n$ small triangles, and we delete a vertex from each of these.
\end{proof}
\moreINT[A construction of Erd\H{o}s:] $T(n)\geq1/(2(n-1)^2)$ when $n$ is prime.

\begin{proof} Graph the parabola $y=x^2$ in $(\Z/n\Z)^2$ after embedding $(\Z/n\Z)^2$ in $[0,n-1]^2$ as the lattice points. As $n$ is prime, no three points are collinear, and each triangle has integer coordinates, so has area a multiple of $1/2$.
\end{proof}
\end{INT}
Note that in fact, $T(n)=\Omega(\log n/n^2)$.
\item Given a bounded measurable subset $C\subset \R^d$, the \textbf{packing number} of $X$ is 
\[\delta(C):=\mu(C)\lim_{x\rightarrow\infty} f(x)/x^d\]
where $f(x)$ is the largest number of copies of $C$ that fit inside $[0,x]^d$.
\begin{INT}[Thm 3.4.1:]
Let $C$ be convex and centrally symmetric around the origin. Then $\delta(C)\geq2^{-d-1}$.
\begin{proof}
We'd like to choose the centres of the copies of $C$ randomly inside the box $B=[0,x]^d$. Choosing $p,q\in B$ at random, $(C+p)$ intersects $(C+q)$ \Iff $p-q\in2C$, by central symmetry. Conditioning on fixed $q$, this occurs with probability at most $\mu(2C)/x^{d}$, and thus the unconditional probability of an intersection is at most $(2/x)^d\mu(C)$.

\INDENT Now choose $n$ points in $B$ at random, and put a copy of $C$ at each. Then average number of intersections is at most $(n^2/2)(2/x)^d\mu(C)$, so removing some, we obtain $n-(n^2/2)(2/x)^d\mu(C)$ points. Choose $n$ to maximise this quantity, and note that the fact that some of the copies may hang over the side of the box is insignificant in the limit.
\end{proof}
\moreINT[A greedy algorithm:] By simply adding new copies of $C$ until we can no longer go on, we see that $\delta(C)\geq 2^{-d}$.
\begin{proof}
Note that if we choose $p_1,\ldots,p_m$ a maximal subset of $B$ such that the $C+p_i$ are disjoint. Then by maximality, there can be no point $q$ such that $q-p_i\notin 2c$ for all $i$ (else we can add $q$ to the collection). Thus $n$ copies of $2C$ cover $B$, from which we can derive the result.
\end{proof}
\end{INT}
\item \textbf{Recolouring:}  recall that we wrote $m(n)>m$ \Iff every $n$-uniform hypergraph with $m$ edges is \textbf{2-colourable}, i.e.\ has \textbf{property B}.
\begin{INT}[Thm 3.5.1:]
If there is a probability $p$ such that $k(1-p)^n+k^2p<1$, then $m(n)>2^{n-1}k$.
\moreINT[Cor 3.5.2:] $m(n)=\Omega(2^n\sqrt{n/\log n})$.
\begin{proof}[Proof of 3.5.2:]
It's good enough to find the largest $p$ such that $k^2p<1/2$ and then plug it into $k(1-p)^n<1/2$, to see that $k$ satisfying the hypotheses of 3.5.1 can be chosen fairly large.
\end{proof}
\begin{proof}[Proof of 3.5.1:]
It's enough to fix a hypergraph with $2^{n-k}$ edges, and prove that its vertices can be 2-coloured.
We give three sources of randomness. For each vertex, we flip a fair `red'/`blue' coin and a $(p,1-p)$ `recolour'/`don't recolour' coin. We also randomly order the vertices. All this happens once and for all at the start.

\INDENT The idea is as follows. Colour the vertices according to their red/blue coins. Let $D$ be the set of `dangerous' vertices --- those that lie in a monochromatic edge. Go through the vertices in $D$ in their (random) order and recolour the coin in question \Iff it is `still dangerous' (it still lies in a monochromatic edge) and its second coin displays `recolour'. Note here that coins which are not initially considered dangerous are \emph{never} candidates for recolouration. The point is that we can bound the failure probability by $k(1-p)^n+k^2p$.

\INDENT We'll bound the probability that there is a red edge at the end of the process. The point is that there is a red edge \Iff one of the following occur, for $e\in E$:
\begin{itemise}
\item[$A_e$:] $e$ was initially red and remained so forever ($\Prob[A_e]=2^{-n}(1-p)^n$).
\item[$C_e$:] $e$ was not initially red but became so.
\end{itemise}
Now if $C_e$ occurs, one can see that `$e$ blames $f$' (written $B_{ef}$), for some $f\in E$. That is:
\begin{itemise}
\item $e$ ends up red; $f$ starts out blue;
\item $e\cap f=\{v\}$ --- the two edges share exactly one vertex, $v$;
\item $v$ is the last vertex of $e$ to switch to red;
\item when $v$ changed colour, $f$ was still blue.
\end{itemise}
Now suppose that $e,f$ are edges with $e\cap f=\{v\}$. We wish to bound $\Prob[B_{ef}]$. Fix the order $\sigma$ on the vertices, and calculate conditionally. Write $i$ (resp.\ $j$) for the number of vertices of $e$ (resp.\ $j$) which precede $v$ in $\sigma$. Then
\[\Prob[B_{ef}|\sigma]\leq\left[\frac{1}{2}\cdot p\right] \left[2^{-n+1}\right] \left[(1-p)^j\right] \left[2^{-(n-i+1)}\right] \left[\left(\frac{1}{2}+\frac{1}{2}\cdot p\right)^i\right]\]
Here the factors are, in order:
\begin{enumerate}\squishlist
\item $v$ must have `blue' and `recolour';
\item all other vertices of $f$ must have `blue';
\item the vertices of $f$ which precede $v$ must have `don't recolour';
\item the vertices of $e$ after $v$ must have `red';
\item the vertices of $e$ before $v$ must either have `red' or have both `blue' and `recolour'.
\end{enumerate}
In particular, we have
\[\Prob[B_{ef}]\leq 2^{1-2n}p\Exp[(1+p)^i(1-p)^j].\]
The result will follow if we can see that the expectation is at most one. This is so, by the following trick. Let $I$ be a set of size $n-1$, and let $\phi:T\to e-\{v\}$ and $\psi:T\to f-\{v\}$ be bijections. Let $I_i$ (resp.\ $J_i$) be the indicator random variable for the event that $\phi(i)$ (resp.\ $\psi(i)$) precedes $v$. Then
\begin{alignat*}{2}
\Exp[(1+p)^i(1-p)^j]&=\Exp\left[\prod (1+p)^{I_i}(1-p)^{J_i}\right]%&\qquad&\text{()}
\\&=\prod \Exp\left[(1+p)^{I_i}(1-p)^{J_i}\right]\leq\prod1
&\qquad&\text{(independence)}
\end{alignat*}
As there are less than $(2^{n-1}k)^2$ pairs of edges, the probability of any failure of type $C_e$ is at most $k^2p/2$. Similarly, the probability of any $A_e$ occurring is at most $k(1-p)^n/2$.
\end{proof}
\end{INT}
\item \textbf{There's some stuff here about `continuous time', which I haven't read.}
\end{itemise}
\subsection*{The Probabilistic Lens: High Girth and High Chromatic Number}
The \textbf{girth} of a graph $G$ is the size of its shortest cycle. We'll write $\alpha(G)$ for the independence number (the size of the largest independent set). The \textbf{chromatic number} $\chi(G)$ is the least number of colours needed to colour the vertices so that no edge is monochromatic.
\begin{INT}[Erd\H{o}s (1959):]
For all $k,l$ there exists a graph $G$ with $\text{girth}(G)>l$ and $\chi(G)>k$.
\begin{proof}
The idea is as follows. Fix $\theta<1/l$, let $p=n^{\theta-1}$, and let $G\sim G(n,p)$. This graph, for large $n$, often has few ($<n/2$) cycles shorter than $l$, and a small ($<(3\log n)/p$) independence number. Removing a vertex for each short cycle, we have a graph $G^*$ of size at least $n/2$ with girth at least $l$, and small independence number. As chromatic number and independence number are `inverse' ($\chi(G^*)\geq|G^*|/\alpha(G^*)$), we see that the resulting graph also has high chromatic number (for $n\gg0$).

 Then if $X$ is the average number of cycles of length at most $l$:
\[\Exp[X]=\sum_{i=3}^l\frac{n(n-1)\cdots (n-i+1)}{2\cdot i}p^i\leq \sum_{i=3}^l\frac{n^{\theta i}}{2i}=o(n)\textup{\qquad so that\qquad}\Prob[X\geq n/2]=o(1).\]
Let $x=\lceil (3\log n)/p\rceil$, so that
\[\Prob[\alpha(G)\geq x]\leq{n\choose x}(1-p)^{x\choose2}<\left[ne^{-p(x-1)/2}\right]^x=o(1)\]
Then for $n\gg0$ some choice of $G$, we have $\textup{girth}(G^*)\geq l$, $|G^*|\geq n/2$, and $\alpha(G^*)\leq 3n^{1-\theta}\log n$. As $n$ grows even larger, we get $\chi(G^*)$ as large as we like.
\end{proof}
\end{INT}
\end{chapter3}
\begin{chapter4}
\section*{\S4: The second moment}
\begin{itemise}
\item \begin{INT}[Chebyshev's Inequality:]
$\Prob[|X-\mu|\geq\lambda\sigma]\leq\lambda^{-2}$.
\begin{proof}
$\sigma^2=\Exp[(X-\mu)^2]\geq\lambda^2\sigma^2\Prob[|X-\mu|\geq\lambda\sigma].$
\end{proof}
\item If $X=\sum X_i$, then $\Var[X]=\sum_{i,j}\Cov[X_i,X_j]$. If the $X_i$ are indicator random variables, then $\Var[X_i]\leq\Exp[X_i]$, so that $\Var[X]\leq\Exp[X]+\sum_{i\neq j}\Cov[X_i,X_j]$.
\end{INT}
\item Let $\nu(n)$ be \textbf{the number of distinct primes dividing $n$}. Then `almost all' $n$ have $\nu(n)$ `close to' $\log\log n$.
\begin{INT}[Thm 4.2.1:] Let $\omega(n)\rightarrow\infty$ arbitrarily slowly. Then the number of $x$ in $\{1,\ldots,n\}$ such that
\[|\nu(x)-\log\log n|>\omega(n)\sqrt{\log\log n}\]
is $o(n)$.
\begin{proof}
Choose such $x$ at random. Let $X_p$ be the indicator random variable for $p|x$ for $p\leq n^{1/10}$ and let $X=\sum X_p$. Now
\[\Exp[X_p]=1/p+O(1/n)\textup{\qquad so that\qquad}\Exp[X]=\sum_{p\leq M}\left(\frac{1}{p}+O\left(\frac{1}{n}\right)\right)=\log\log n+O(1).\]
And similarly we can see that
\[\sum_{p\leq M}\Var[X_p]=\log\log n+O(1)\textup{\qquad and\qquad}\Cov[X_p,X_q]\leq\frac{1}{n}\left(\frac{1}{p}+\frac{1}{q}\right).\]
This shows that $\sum_{p\neq q}\Cov[X_p,X_q]\in[-o(1),o(1)]$, so that the covariances are negligible. Now we can apply Chebyshev's inequality:
\[\Prob\left[|X-\log\log n|>\omega(n)\sqrt{\log\log n}\right]<\omega(n)^{-2}+o(1)\rightarrow0.\qedhere\]
\end{proof}
\moreINT[Thm 4.2.2] is a refinement of this result.
\end{INT}
\item \textbf{Non-negative integer valued random variables} have some handy properties. Let $Y$ and $X_1,X_2,\ldots$ be non-negative integer valued random variables, whose `limit' (meaningless) we denote by $X$. When we make a claim about $X$, we mean that the claim tends towards being correct as $n\rightarrow \infty$.
\begin{INT}
$\Prob[Y>0]\leq\Exp[Y]$.
\moreINT If $\Exp[X_n]\rightarrow0$ then $X=0$ almost always.
\moreINT[Thm 4.3.1:] $\Prob[Y=0]\leq\frac{\Var[Y]}{\Exp[Y]^2}$.
\begin{proof}
Chebyshev's inequality gives a stronger result:
$\Prob[|Y-\mu|\geq\mu]\leq\frac{\sigma^2}{\mu^2}$.
\end{proof}
\moreINT[Cor 4.3.2 \& 4.3.3:] If $\Var[X_n]=o(\Exp[X_n]^2)$ then, almost always:
\[X>0,\textup{\qquad and\qquad}X\sim \Exp[X].\]
\begin{proof}
The first is just a restatement of 4.3.1. For the second, note that we could have written, for any $\epsilon>0$, 
$\Prob[|Y-\mu|\geq\epsilon\mu]\leq\frac{\sigma^2}{\epsilon^2\mu^2}\rightarrow0$.
\end{proof}
\end{INT}
\item \textbf{Sums of indicator random variables:} Let $X=X_1+\cdots+X_m$ be a sum of indicator random variables for events $A_i$. Write $i\sim j$ \Iff $i\neq j$ and $A_i$ and $A_j$ are not independent, in which case, $\Cov[X_i,X_j]\leq\Prob[A_i\wedge A_j]$. Define
\[\Delta:=\sum_{i\sim j}\Prob[A_i\wedge A_j],\textup{\qquad so that\qquad}\Var[X]\leq\Exp[X]+\Delta.\]
\begin{INT}[Cor 4.3.4:]
If $\Exp[X]\rightarrow\infty$ and $\Delta=o(\Exp[X]^2)$ then, almost always:
\[X>0,\textup{\qquad and\qquad}X\sim \Exp[X].\]
\end{INT}
We say that the indicators $X_1,\ldots,X_n$ are \textbf{symmetric} if for each $i\neq j$ there's a measure-preserving automorphism of the probability space sending $A_i$ to $A_j$. Then
\[\Delta=\Delta^*\cdot\Exp[X],\textup{\qquad where\qquad}\Delta^*:=\sum_{j\sim i}\Prob[A_j|A_i]\textup{\quad(independent of $i$)}.\]
\begin{INT}[Cor 4.3.5:]
If $\Exp[X]\rightarrow\infty$ and $\Delta^*=o(\Exp[X])$ then, almost always:
\[X>0,\textup{\qquad and\qquad}X\sim \Exp[X].\]
In particular, this applies when ($\Exp[x]\rightarrow\infty$, and) sufficiently many of the $A_i$ and $A_j$ are independent (or have low probability), that conditioning on one of the $A_i$ doesn't change the expected number of events $A_j$ that occur significantly.
\end{INT}
\item %A \textbf{property of graphs} is a family of graphs closed under isomorphism. 
A function $r$ is a \textbf{threshold function} for a property $P$ of graphs if whenever $p(n)\ll r(n)\ll q(n)$ then $G(n,p)$ almost always fails to have $P$, and $G(n,q)$ almost always has $P$.

The \textbf{clique number} $\omega(G)$ of a graph $G$ is the maximal number of vertices of a clique, where a \textbf{clique} is a subgraph which is a complete graph.
\begin{INT}[Thm 4.4.1:]
The property $\omega(G)\geq4$ has threshold function $n^{-2/3}$.
\end{INT}
\begin{proof}
For each $S$ a set of $4$ vertices, let $A_S$ be the event that $S$ is a clique, indicated by $X_S$. Let $X=\sum X_S$, so that $\Exp[X]={n\choose 4}p^6\sim n^4p^6/24$.

\INDENT Suppose first that $p(n)\ll n^{-2/3}$. Then $\Exp[X]=o(1)$ so that $X=0$ almost surely.

\INDENT Suppose instead that $p(n)\gg n^{-2/3}$, in which case $\Exp[X]\rightarrow\infty$. Then we'd like to show that $\Delta^*=o(\Exp[X])$, so that $X\sim \Exp[X]$ almost surely, completing the proof. But this is easy, as there are very few dependencies, as $S\sim S'$ \Iff $|S\cap S'|$ is 2 or 3, and for fixed $S'$, this happens for less than $n^2$ and $n$ $S'$ respectively. We find
\[\Delta^*\leq n^2p^5+np^3=\left[\frac{1}{n^2p^1}+\frac{1}{n^3p^3}\right]n^4p^6=o(n^4p^6)=o(\Exp[X]).\qedhere\]
\end{proof}
\item The \textbf{density} $\rho(H)$ of a graph $H=(V,E)$ is simply $|E|/|V|$. We say that $H$ is \textbf{balanced} if every subgraph of $H$ has density no greater than that of $H$.
\begin{INT}[Thm 4.4.2:]
Let $H$ be a balanced graph with $v$ vertices and $e$ edges. Let $A(G)$ be the event that $H$ is a subgraph of $G$ (not necessarily induced). Then $p=n^{-v/e}$ is a threshold function for $A$.
\begin{proof}
For each $S$ a set of $v$ vertices, let $A_S$ be the event that $G|_S$ contains $H$ as a subgraph, and let $X_S$ be the indicator. Let $X=\sum X_S$.

\INDENT To see that this almost never occurs for $p\ll n^{-v/e}$, we note that $\Exp[X]=o(1)$ in this case.

\INDENT Suppose instead that $p\gg n^{-v/e}$. Then, like last time, $\Exp[X]=\Theta(n^vp^e)\rightarrow\infty$, and
\[\Delta^*=\sum_{i=2}^{v-1}\sum_{|T\cap S|=i}\Prob[A_T|A_S]\]
Now suppose that $|T\cap S|=i$ and we know that every edge possible appears in $S$. Any possible appearance of $H$ with vertices in $T$ (i.e.\ bijection of the vertices $T$ with those of $H$, of which there are $v!$ in some sense) has at most $ie/v$ edges wholly contained in $S$, as $H$ is balanced. Thus there is at most a probability of $p^{e-ie/v}$ that this happens. Thus we have $\Prob[A_T|A_S]=O(p^{e-ie/v})$. As there are $O(n^{v-i})$ choices of such $T$:
\[\Delta^*=\sum_{i=2}^{v-1}O(n^{v-i}p^{e-ie/v})=\sum_{i=2}^{v-1}O(n^{v}p^{e})^{1-i/v}=o(n^vp^e)\text{\qquad(as $n^vp^e\rightarrow\infty$).}\]
This is $o(\Exp[X])$, so we're done.
\end{proof}
\end{INT}
\item \textbf{There's some more stuff here, to which I should return.}
\item \textbf{Clique number:} Fix edge probability $1/2$ and consider the clique number $\omega(G)$. \textit{The following is quite subtle.} Let $f(k)$ be the expected number of $k$-cliques, ${n\choose k}2^{-{k\choose2}}$. This drops below one at $k\sim 2\log_2n$.
\begin{INT}[Thm 4.5.1:] Let $k=k(n)$ satisfy $k\sim 2\log_2n$ and $f(k)\rightarrow\infty$ [so that the average number of $k$-cliques is unbounded, but the average number of $rk$-cliques is less than one for some fixed $r$]. Then almost always $\omega(G)\geq k$.
\begin{proof}
For each $k$-set $S$, let $A_S$ be the event ``$S$ is a clique'', and $X_S$ the indicator. Let $X=\sum X_S$. We need to see that almost always $X>0$. Well $E[X]=f(k)\rightarrow\infty$, so we consider:
\[\Delta^*=\sum_{T\sim S}\Prob[A_T|A_S]=\sum_{i=2}^{k-1}{k\choose i}{n-k\choose k-i}2^{-\left({k\choose2}-{i\choose2}\right)}\]
Detailed calculation shows that $\Delta^*$ is $o(\Exp[X])$, and we're done.
\end{proof}
\moreINT[Cor 4.5.2:] There exists $k=k(n)$ so that $\Prob[\omega(G)=k\textup{ or }k+1]\rightarrow1$.
\begin{proof}
Let $k(n)$ be as large as possible so that $f(k(n))\geq\sqrt n$. Observe that for $k\sim 2\log_2 n$, $f(k+1)/f(k)=n^{-1+o(1)}$, so that for $n\gg0$, the function $f$ is decreasing by ratios of at least, say, $n^{.9}$. Then $f(k(n)+2)<n^{-.4}\rightarrow0$. Then, almost surely there is no $(k(n)+2)$-clique, and by 4.5.1 there is almost surely a $k(n)$-clique.
\end{proof}
\end{INT}
\item \textbf{Distinct sums:} Let $f(n)$ denote the largest $k$ for which there is a set $\{x_1,\ldots,x_k\}\subset\{1,\ldots,n\}$ with \textbf{distinct sums}. That is, for which the sums $\sum_{i\in X}x_i$, for $S\subset\{1,\ldots,k\}$, are distinct.
\begin{INT}[Thm 4.6.1:]
$f(n)<\log_2n+\frac{1}{2}\log_2\log_2n+O(1)$.\footnote{Noting that $2^{f(n)}<nf(n)$, we can show that $f(n)<\log_2n+\log_2\log_2n+O(1)$.}
\begin{proof}
Fix a set $\{x_1,\ldots,x_k\}$ in $\{1,\ldots,n\}$ with distinct sums, and let $X$ the sum of a random subset. Then:
\[\mu:=\Exp[X]=\frac{x_1+\cdots+x_k}{2}\textup{\qquad and\qquad}\sigma^:=\Var[X]=\frac{x_1^2+\cdots+x_k^2}{4}\leq\frac{n^2k}{4}\]
Chebyshev's inequality gives the left hand inequality, and as $X$ takes any given value with probability at most $2^k$, we have the right hand inequality, in:
\[1-\lambda^{-2}\leq
\Prob\left[|X-\mu|<\lambda n\sqrt{k}/2\right]\leq
2^{-k}\left(\lambda n\sqrt{k}+1\right)\textup{\quad which implies\quad }
n\geq\frac{2^k\left(1-\lambda^{-2}\right)-1}{\lambda\sqrt k}.\]
This holds for any $\lambda>0$, and choosing any $\lambda>1$, for some fixed $\gamma$ we have $n\geq 2^kk^{-1/2}\gamma$. Now apply ``$\log_2(\DASH)+\frac{1}{2}\log_2\log_2(\DASH)$'' to both sides, and note that $\textup{RHS}>k-\textup{constant}$.
\end{proof}

\end{INT}
\end{itemise}
\subsection*{The Probabilistic Lens: Hamiltonian Paths in a tournament}
Let $P(n)$ denote the \textbf{maximum number of directed Hamiltonian paths in a tournament} of size $n$. We saw that $P(n)\geq n!/2^{n-1}$, and Szele showed that
\[\frac{1}{2}\leq\lim_{n\rightarrow\infty}\left(\frac{P(n)}{n!}\right)^{1/n}\leq2^{-3/4}.\]
We've already seen that the limit exceeds $1/2$, and we will see that the limit equals $1/2$ if we can prove the following.
\begin{INT}[Thm 1:] There exists a positive constant $c$ such that for all $n$,
$P(n)\leq cn^{3/2}\frac{n!}{2^{n-1}}$.
\begin{proof}
For any tournament $T$ with $n$ players, let $P(T)$ be the number of Hamiltonian paths, $C(T)$ be the number of Hamiltonian cycles, and $F(T)=\textup{Per}(A_T)$ be the number of (vertex disjoint) cycle covers, the permanent of the adjacency matrix. If $r_i$ is the outdegree of the $i^\textup{th}$ player, then $\sum r_i={n\choose2}$. By Br\'egman's theorem, we have $F(T)\leq\prod_{i=1}^n(r_i!)^{1/r_i}=:B(\underline{r})$, and we would like to make this bound independent of the $r_i$. It turns out that $B(\underline{r})$ is maximised when the $r_i$ are as close together as possible (i.e.\ all either $\lfloor{n\choose2}/n\rfloor$ or $\lceil{n\choose2}/n\rceil$), and we obtain (post tedium):
\[C(T)\leq F(T)\leq (1+o(1))\frac{\sqrt\pi}{e\sqrt2}n^{3/2}\frac{(n-1)!}{2^n}.\]
At this point we are finally called on to use the probabilistic method. Given $S$ an $n$-tournament, form $T$ a (random) $(n+1)$-tournament by adding a vertex to $S$ and directing the new edges randomly. Now $\Exp[C(T)]=P(S)/4$, so that there is some $T$ such that $C(T)\geq P(S)/4$. But by the above equation (which holds for $n$-tournaments), we get what we need:
\[P(S)\leq 4C(T)\leq 4(1+o(1))\frac{\sqrt\pi}{e\sqrt2}(n+1)^{3/2}\frac{n!}{2^{n+1}}=O\left(n^{3/2}\frac{n!}{2^{n-1}}\right).\qedhere\]

\end{proof}
\end{INT}
\end{chapter4}
\begin{chapter5}
\section*{\S5: The local lemma}
\begin{itemise}
\item 
Suppose that there is a criminal to be executed. There is a finite firing squad $\{F_i\}$, and each member $F_j$ is holding either a real or a fake gun, with probabilities $x_j$ and \smash{$1-x_j$}, independently of the other members. Each $F_j$ cannot tell whether or not his gun is real, but he can tell whether or not any other gun that he sees is real.

\INDENT None of the members of the squad want to do the deed. They will only pull the trigger if they cannot see any other squad member who has a real gun. Thus the probability that $F_i$ kills the criminal is \smash{$x_i\prod_j(1-x_j)$}, where the product is taken over those $j$ such that $F_j$ is standing close enough for $F_i$ to see.

\INDENT The probability that the criminal dies is of course at most $\prod(1-x_i)$, as if they all get fake guns, he'll survive.

\item The Lov\'asz Local Lemma states that if the probability that $F_j$ kills the criminal are lower than $x_i\prod(1-x_j)$, then the probability that the criminal dies is still at most $\prod(1-x_i)$.
\begin{INT}[Thm 5.1.1 {[the local lemma]}:]
Let $A_1,\ldots,A_n$ be events. Let $D=(V,E)$ be a dependency digraph on $V=\{1,\ldots,n\}$, so that $A_i$ is mutually independent\footnote{An event $A$ is mutually independent of a collection $B_1,\ldots,B_r$ of events if for any subset $S\subset\{1,\ldots,n\}$, $\Prob[A]=\Prob[A|\bigvee_{i\in S} B_i]$. Note that the $B_i$ \textbf{do not} have to be mutually independent.} of the set of $A_j$ such that $(i,j)\notin E$. Suppose that there are $x_i\in[0,1)$ such that \smash{$\Prob[A_i]\leq x_i\prod_{(i,j)\in E}(1-x_j)$}. Then the probability that no $A_i$ occurs is at least \smash{$\prod(1-x_i)$}.
\moreINT[A stronger version:]
Let $A_1,\ldots,A_n$ be events, and let $D=(V,E)$ be \emph{any} digraph on $V=\{1,\ldots,n\}$. Make no independence assumptions. Suppose that for each $i$, and every set $S$ of vertices not hit by $i$, we have
\[\Prob\left[A_i\,\middle|\,\bigwedge_{s\in S}\overline{A_s}\right]\leq x_i\prod_{(i,j)\notin E}(1-x_j).\]
Then  the probability that no $A_i$ occurs is at least \smash{$\prod(1-x_i)$}.


\moreINT[Cor 5.1.2 {[the symmetric case]}:]
Let $A_1,\ldots,A_n$ be events. Suppose that each is mutually independent of a set of all but $d$ others, and $\Prob[A_i]\leq p$. Then if $ep(d+1)\leq 1$, there is a nonzero probability that none of the \smash{$A_i$} occurs.
\begin{proof}
Take \smash{$x_i=\frac{1}{d+1}$}. Need \smash{$p\leq \frac{1}{d+1}\left(1-\frac{1}{d+1}\right)$}.
\end{proof}
\end{INT}
\item \textbf{Property B:} Recall that a hypergraph has property B if its vertices are two-colourable.
\begin{INT}[Thm 5.2.1:]
Let $H$ be a hypergraph in which each edge has at least $k$ elements, and each edge meets at most $d$ other edges. If $e(d+1)\leq2^{k-1}$ then $H$ has property B.
\begin{proof}
The events are that a given edge is monochromatic. They occur with probability at most $2^{1-k}$. Each depends on at most $d$ others.
\end{proof}
\moreINT[Corollary:] For $k\geq9$, and $k$-uniform $k$-regular\footnote{A hypergraph is $k$-uniform if each edge has $k$ vertices, and $k$-regular if each vertex has degree $k$.} hypergraph has property B.
\begin{proof}
For each edge meets at most $k(k-1)$ other edges, so we can take $d=k(k-1)$ above.
\end{proof}
\end{INT}
\item \textbf{$k$-colourings of the reals:} Given a $k$-colouring of $\R$, call $T\subset\R$ \textbf{multicolored} if it contains elements of every colour.
\begin{INT}[Thm 5.2.2:]
Let $m$ and $k$ be two positive integers satisfying (think: fix $k$, choose $m$ large enough)
\[e(m(m-1)+1)k\left(1-k^{-1}\right)^m\leq1.\]
Then for any set $S$ of $m$ real numbers there is a $k$-colouring so that each translation $x+S$ is multicolored.
\begin{proof}
First, fix finite $X\subset\R$, and find a colouring such that the translations $x+S$ ($x\in X$) are multicoloured. Colour every element of \smash{$\bigcup_{x\in X}(x+S)$} at random, and let $A_x$ be the event that the translate by $x$ fails to be multicoloured. Each $A_x$ is independent of all but at most $m(m-1)$ of the others. Note that $\Prob[A_x]\leq k\left(1-k^{-1}\right)^m$, and apply the lemma.

\INDENT Extend this to $X=\R$ by the following argument. The set $\{1,\ldots,k\}^{\R}$ is compact, and the set $C_x$ of colourings such that $x+S$ is multicoloured is closed. [A basis of open sets consists of the sets of colourings whose values are prescribed at a given bunch of places, so that $C_x$ and its complement are both a finite union of open subsets.] As any finite intersection of the $C_x$ is nonempty, so is the intersection of all $C_x$.
\end{proof}
\end{INT}
\item \textbf{Lower bounds for Ramsey numbers:} The Ramsey Number $R(k,l)$ is the least $n$ such that any two-colouring of the edges of $K_n$ contains a red $K_k$ of blue $K_l$.
\begin{INT}[Prop 5.3.1:]
If $e{k\choose 2}{n-2\choose k-2}\cdot2^{1-{k\choose2}}<1$ then $R(k,k)>n$. In particular \[R(k,k)>(\sqrt2/e)(1+o(1))k2^{k/2}.\]
\begin{proof}
For $S$ a set of $k$ vertices, let $A_S$ be the event that the induced complete graph is monochromatic. $\Prob[A_S]=2^{1-k}$, and $A_S$ is independent of all but less than ${k\choose 2}{n-2\choose k-2}$ other events (this being the total number of `dependencies' any edge of $S$ has with other events).
\end{proof}
This was only a small improvement, as there are so many dependencies --- \smash{${k\choose 2}{n-2\choose k-2}$} is a large proportion of the total number of events. One does better with such numbers as $R(k,3)$.
\end{INT}
\item %\textbf{Coverings: $k$-fold and indecomposable:}
A family $\calF$ of open unit balls in $\R^3$ is a \textbf{$k$-fold covering} if every point is contained in $k$ balls. It is \textbf{decomposable} if it can be partitioned into two pairwise disjoint families which cover $\R^3$.
\begin{INT}
One might be surprised to hear that it is harder to decompose $k$-fold coverings which are not too wasteful than ones which are fairly regular:
\moreINT[Thm 5.4.1:]
Suppose that $\calF$ is a $k$-fold covering. Suppose that no point of $\R^3$ is contained in more than $t$ balls. If $et^32^{18}/2^{k-1}\leq1$, then $\calF$ is decomposable.
\begin{proof}
The idea will be to randomly colour the family, and for all $x\in\R^3$, let $A_x$ be the event that $x$ is not contained in a ball of each colour. We can only do this for finite collections of $x\in\R^3$ at a time, but can then extend our result to all points at once by a compactness argument.

\INDENT Of course $\Prob[A_x]\leq1/2^{k-1}$. We thus only need to show that the event $A_{x_i}$ is mutually independent from all but $t^32^{18}-1$ of the other events $A_{x_j}$. We can of course assume that none of the events $A_{x_1},\ldots,A_{x_n}$ coincide, which occurs when two of the $x_i$ are contained in exactly the same balls.

\INDENT Now $t^32^{18}=(4^3t)^3$ is an upper bound for the number of regions $4^3t$ unit balls can cut $\R^3$ into. Any ball relevant to $x_i$ lies entirely within the ball of radius 4 around $x_i$, which has volume $4^3$ times the volume of a unit ball, and is being covered at most $t$-fold.
\end{proof}
\end{INT}
\item \textbf{The linear arboricity of graphs:} the least number of linear forests in a graph $G$ required in order to cover the edges is called the \textbf{linear arboricity} $\textup{la}(G)$ of $G$.
\begin{INT}[The linear arboricity conjecture:]
The linear arboricity of every $d$-regular graph is $\lceil (d+1)/2\rceil$.
\end{INT}
\item \textbf{Latin transversals:} Let $A$ be an $n\times n$ integer matrix. A \textbf{latin transversal} of $A$ is a permutation $\pi$ such that the entries $a_{i\pi(i)}$ are distinct.
\begin{INT}[Thm 5.6.1:]
Suppose that $k\leq(n-1)/(4e)$ and suppose that no integer appears in more than $k$ entries of $A$. Then $A$ has a latin transversal.
\begin{proof}
\textbf{Read me.}
\end{proof}
\end{INT}
\item \textbf{The algorithmic aspect: Read me.}
\end{itemise}
\subsection*{The Probabilistic Lens: Directed Cycles}
Let $D$ a a simple digraph with minimum outdegree $\delta$ and maximum indegree $\Delta$.
\begin{INT}[Thm 1:]
If $e(\Delta\delta+1)(1-k^{-1})^\delta<1$ then $D$ contains a (simple) cycle of length a multiple of $k$.
\end{INT}
\begin{proof}
Restrict to a subgraph $(V,E)$ of outdegree precisely $\delta$. The idea will be to $k$-colour the graph by a function \smash{$c:V\to\Z/k\Z$}, and show that with nonzero probability for all $v\in V$ there's $(v,v')\in E$ such that $c(v')=c(v)+1$.

\INDENT For each $v\in V$, let $A_v$ be the event that there is no such $v'$. $A_v$ is mutually independent of the set of $A_u$ such that $v$ doesn't see $u$, and $v$ and $u$ can't both see any third vertex. $v$ sees $\delta$ vertices, each of which is seen by at most $\Delta-1$ others. So there are at most $\delta+\delta(\Delta-1)$ events of which $A_v$ is not mutually independent. Note $\Prob[A_v]=(1-k^{-1})^\delta$ and apply the local lemma.
\end{proof}
\end{chapter5}
\begin{chapter6}
\section*{\S6: Correlation Inequalities}
\begin{itemise}
\item Let $G\sim G(n,p)$ be a random graph. Let $H$ be the event that $G$ is \textbf{hamiltonian} (admits a hamiltonian cycle). Let $P$ be the event that $G$ is planar. It would seem that $\Prob[P|H]\leq\Prob[P]$, so that $\Prob[P\sprod H]\leq\Prob[P]\Prob[H]$.

\INDENT This is in fact true, and a special case of the FKG inequality. Another example is the following. Suppose that $\calA,\calB$ are order ideals of $\{1,\ldots,n\}$. Then $|\calA\cap \calB|\cdot2^n\geq|\calA|\cdot |\calB|$.
\item \textbf{The four functions theorem:} let $N=\{1,\ldots,n\}$. For a function $\phi:\calP(N)\to\R^+$, define $\phi:\calP(\calP(N))\to\R^+$ by $\calA\mapsto\sum_{A\in\calA}\phi(A)$. That is, given a function on subsets of $N$, extend it to sets of subsets by summation.
\begin{INT}[Thm 6.1.1 {[four functions]}:] Let $\alpha,\beta,\gamma,\delta$ be functions $\calP(N)\to\R^+$, and suppose that
\[\alpha(A)\beta(B)\leq\gamma(A\cup B)\delta(A\cap B)\textup{\quad for all $A,B\in\calP(N)$}.\]
Then
\[\alpha(\calA)\beta(\calB)\leq\gamma(\calA\cup \calB)\delta(\calA\cap \calB)\textup{\quad for all $\calA,\calB\subseteq\calP(N)$}.\]
\end{INT}
Actually this result is better stated on finite distributive lattices. A \textbf{lattice} is a poset in which any two elements have a unique minimal upper bound, the \emph{meet} $x\vee y$, and a unique maximal lower bound, the \emph{meet} $x\sprod y$. A lattice is \textbf{distributive} if 
\[x\sprod(y\vee z)=(x\sprod y)\vee (x\sprod z)\textup{\qquad or equivalently\qquad }x\vee(y\sprod z)=(x\vee y)\sprod (x\vee z).\]
As \emph{every finite distributive lattice is a sublattice of some $\calP\{1,\ldots,n\}$}:
\begin{INT}[Cor 6.1.2 {[four functions]}:] Let $\alpha,\beta,\gamma,\delta$ be functions $L\to\R^+$, for $L$ a finite distributive lattice. Suppose that
\[\alpha(A)\beta(B)\leq\gamma(A\vee B)\delta(A\sprod B)\textup{\quad for all $A,B\in\calP(N)$}.\]
Then
\[\alpha(\calA)\beta(\calB)\leq\gamma(\calA\vee \calB)\delta(\calA\sprod \calB)\textup{\quad for all $\calA,\calB\subseteq\calP(N)$}.\]
\moreINT[Cor 6.1.3:] For $X,Y\subset L$, where $L$ is a finite distributive lattice:
\[|X|\cdot|Y|\leq|X\vee Y|\cdot|X\sprod Y|.\]
\begin{proof}
Set the four functions to be identically 1.
\end{proof}
\moreINT[Cor 6.1.4:]
Let $\calA$ be a family of subsets of a finite set $N$. Then $|A\setminus A|\geq|A|$, where we define $|A\setminus A|=\{F\setminus F'\,:\, F,F'\in\calA\}$.
\begin{proof}
Let $L$ be the lattice of subsets of $N$. Let $\calB=\{N\setminus A\,:\, A\in\calA\}$. Then:
\[|\calA|^2=|\calA|\cdot|\calB|\leq|\calA\cup\calB|\cdot|\calA\cap\calB|=|\calA\setminus\calA|^2.\qedhere\]
\end{proof}
\end{INT}
\item \textbf{The FKG inequality:} A function $\mu:L\to\R^+$ is called \textbf{log-supermodular} if:
\[\mu(x)\mu(y)\leq\mu(x\vee y)\mu(x\sprod y)\textup{\quad for all $x,y\in L$,}\]
so that, multiplicatively, $\mu$ gives more weight to more `extreme' points in the lattice, so that one has improving multiplicative returns.
\begin{INT}[Thm 6.2.1 {[FKG]}:]
If $\mu$ is log-supermodular, and $f,g:L\to\R^+$ are increasing, then:
\[\left(\sum\mu(x)f(x)\right)\left(\sum\mu(x)g(x)\right)
\leq\left(\sum\mu(x)f(x)g(x)\right)\left(\sum\mu(x)\right)\textup{\quad i.e.\quad }\langle f\rangle\langle g\rangle\leq\langle fg\rangle,\]
where $\langle f\rangle$ is the expectation of $f$ using the measure on $L$ induced by $\mu$.
\end{INT}
\item \textbf{Monotone properties:} Call a family $\calA$ of subsets of $N=\{1,\ldots,n\}$ \textbf{monotone} decreasing (resp.\ increasing) if it is closed under taking subsets (resp.\ supersets). Define the \textbf{probability} of $\calA$ by $\Prob[\calA]=|\calA|/2^n$, the probability that a random subset of $N$ lies in $\calA$.
\begin{INT}[Prop 6.3.1:]
Let $I_1,I_2\in\calP(\calP(N))$ be increasing and $D_1,D_2\in\calP(\calP(N))$ be decreasing. Then:
\[\Prob[\calI_1\cap\calI_2]\geq\Prob[\calI_1]\cdot\Prob[\calI_2];\quad \Prob[\calD_1\cap\calD_2]\geq\Prob[\calD_1]\cdot\Prob[\calD_2];\quad\textup{and}\quad  \Prob[\calI_1\cap\calD_1]\geq\Prob[\calI_1]\cdot\Prob[\calD_1].\]
\begin{proof}
Let $f:\calP(N)\to\R^+$ be the characteristic function of $I_1$, and $g$ that of $I_2$. These are \emph{increasing}, and so, taking $\mu\equiv2^{-n}$:
\[\Prob[I_1\cap I_2]=\langle fg\rangle\geq\langle f\rangle\langle g\rangle=\Prob[I_1]\cdot\Prob[I_2].\qedhere\]
\end{proof}
\end{INT}
In fact, given a vector \smash{$\frakp=(p_1,\ldots,p_n)\in[0,1]^n$} of probabilities, we can choose a random subset of $N$ but choosing $i$ with probability $p_i$. Let $\Prob_\frakp[\calA]$ be the probability that a random subset chosen in this way is in $\calA$. Then:
\begin{INT}[Thm 6.3.2:]
Let $\calI_1,\calI_2\in\calP(\calP(N))$ be increasing and $\calD_1,\calD_2\in\calP(\calP(N))$ be decreasing. Then:
\[\Prob_\frakp[\calI_1\cap\calI_2]\geq
\Prob_\frakp[\calI_1]\cdot\Prob_\frakp[\calI_2];\quad \Prob_\frakp[\calD_1\cap\calD_2]\geq
\Prob_\frakp[\calD_1]\cdot\Prob_\frakp[\calD_2];\quad\textup{and}\quad  \Prob_\frakp[\calI_1\cap\calD_1]\geq
\Prob_\frakp[\calI_1]\cdot\Prob_\frakp[\calD_1].\]
\begin{proof}
For the altered measure $\mu(A)=\prod_{i\in A}p_i\prod_{j\notin A}(1-p_j)$ is log-supermodular.
\end{proof}
\moreINT[An example:] Fix subsets $A_1,\ldots,A_k$ of $N$; choose random $A\subset N$ by selecting each $i$ independently with some fixed probability. Then
\[\Prob[A\textup{ intersects each }A_i]\geq\prod_{i=1}^k\Prob[A\textup{ intersects }A_i].\]
\moreINT[Thm 6.3.3:] Let $I_1,I_2,D_1,D_2$ be properties of graphs on a fixed set $V$ of vertices, with the $I_i$ increasing and the $D_i$ decreasing, so that adding edges preserves $I_i$, and removing edges preserves $D_i$. Choose a random graph $G$ on vertices $V$ by selecting edges independently with probabilities assigned to each edge individually. Then:
\begin{alignat*}{2}
\Prob[G\textup{ has $I_1$ and $I_2$}]&\geq\Prob[G\textup{ has $I_1$}]\cdot\Prob[G\textup{ has $I_2$}];\\
\Prob[G\textup{ has $D_1$ and $D_2$}]&\geq\Prob[G\textup{ has $D_1$}]\cdot\Prob[G\textup{ has $D_2$}];\\
\Prob[G\textup{ has $I_1$ and $D_1$}]&\leq\Prob[G\textup{ has $I_1$}]\cdot\Prob[G\textup{ has $D_2$}].
\end{alignat*}
This, in particular, proves the claim about being simultaneously Hamiltonian and Planar.
\begin{proof}
Let \smash{$n={|V|\choose 2}$}. Let $\calI_i$ be the set of subsets of $N$ such that the corresponding graph has $I_i$. This is monotone increasing.
\end{proof}
\end{INT}
\item \textbf{Linear extensions of partially ordered sets:} Suppose that $P$ is a poset. A \textbf{linear extension} of $P$ is a total ordering on $P$ compatible with the poset structure. There's a probability space of these, all being equally likely, so that we can consider events such as $x\leq y$, etc.
\begin{INT}[Thm 6.4.1 {[XYZ theorem]}:]
Let $P$ be a poset with $n$ elements. Choose $x,y,z\in P$. Then
\[\Prob[x\leq y\textup{ and } x\leq z]\geq\Prob[x\leq y]\Prob[y\leq z].\]
\begin{proof}
The idea is to consider a random function \smash{$P\overset{\phi}{\to} M$}, where $M=\{1,\ldots,m\}$, for $m$ large. With high probability, this function will be injective, and when it is, it may provide a linear extension of $P$. All possible linear extensions appear with the same frequency.

\INDENT Let $L$ be the set of functions \smash{$P\overset{\phi}{\to} M$}. We'll make this a distributive lattice later. Define:
\[\mu:L\to\R^+\qquad \mu(\phi)=\begin{cases}
1,&\textup{if $\phi$ yields a linear extension};\\
0,&\textup{otherwise}.
\end{cases}\]
so that $\mu$ gives the uniform measure on the linear extensions so obtained. Let $f$ indicate the event that $\phi(x)\leq \phi(y)$, and $g$ indicate the event that $\phi(x)\leq \phi(z)$. Then if $\mu$ is log-supermodular and $f,g$ are increasing, we'll have it that $\langle fg\rangle\geq\langle f\rangle\langle g\rangle$, completing the proof.

\INDENT %Let $P=\{a_1,\ldots,a_n\}$, where $a_1=x$, $a_2=y$ and $a_3=z$.
We say that $\phi\leq\phi'$ if $\phi(x)\geq\phi'(x)$ and $\phi(w)-\phi(x)\leq\phi'(w)-\phi'(x)$ for all $w\in P$. That is, the small functions $\phi$ are those with $\phi(x)$ large, and $\phi_i(w)-\phi_i(x)$ small.
\begin{itemize}\squishlist
\item $(\phi_1\vee\phi_2)(x)=\min\phi_i(x)$, and $(\phi_1\vee\phi_2)(w)$ exceeds this by $\max (\phi_i(w)-\phi_i(x))$.
\item $(\phi_1\sprod\phi_2)(x)=\max\phi_i(x)$, and $(\phi_1\sprod\phi_2)(w)$ exceeds this by $\min (\phi_i(w)-\phi_i(x))$.
%\item The join $\phi_1\vee\phi_2$ then adopts the smaller value $\phi_i(x)$ and then the largest possible gap sizes $\phi_i(w)-(\phi_1\vee\phi_2)(x)$.
%\item The meet $\phi_1\sprod\phi_2$ then adopts the larger value $\phi_i(x)$ and then the smallest possible gap sizes $\phi_i(w)-(\phi_1\vee\phi_2)(x)$.
\end{itemize}
Now if two functions both give a linear extension, then so do their join and meet, so that $\mu$ is log-supermodular. $f$ and $g$ are both obviously increasing.
\end{proof}
\end{INT}
\end{itemise}
\subsection*{The Probabilistic Lens: Tur\'an's theorem}
In a graph $G=(V,E)$, let $d_v$ be the degree of $v\in V$, and $\alpha(G)$ the independence number.
\begin{INT}[Caro and Wei:]
$\alpha(G)\geq\displaystyle\sum(d_v+1)^{-1}$, with equality \Iff $G$ is a disjoint union of cliques.
\begin{proof}
Randomly order the vertices of $G$. The average number ($X$) of vertices smaller than all of their neighbours is this sum.

\INDENT For the comment on equality, it is enough to note that $X$ is constant \Iff $G$ is a {\sc duoc}. If $G$ is a {\sc duoc} then $X$ is constant. If $G$ is not a \textsc{DUOC}, then it contains an induced subgraph, on $V'=\{x,y,z\}\subset V$, of the form $\xymatrix{x\ar@{-}[r]&y\ar@{-}[r]&z}$. Fix an ordering on $V''=V\setminus V'$. Specify that elements of $V'$ are less than elements of $V''$. Then we obtain different values for $X$ from the various orders on $V'$.
\end{proof}
\end{INT}
Consider a graph of the form $(K_{q+1})^{\sqcup m}\sqcup(K_{q})^{\sqcup (m-r)}$. Writing $n$ for its number of vertices and $e$ for its number of edges, denote this graph $G_{n,e}$.
\begin{INT}[Tur\'an's theorem:]
Let $H$ have $n$ vertices and $e$ edges. Then $\alpha(H)\geq m$, with equality \Iff $H\cong G_{n,e}$.
\begin{proof}
Consider the quantity $\sum(d_v+1)^{-1}$, for graphs with a given number of edges ($e=\sum d_v/2$). By convexity, this is minimised when the $d_v$ are as close together as possible, so that $G_{n,e}$ minimises this quantity, at a value of $m$, so:
\[\alpha(H)\geq\sum(d_v+1)^{-1}\geq m.\]
This proves the desired inequality, and if there's equality $\alpha(H)=m$, the inequality of Caro and Wei holds with equality, and we can apply their result.
\end{proof}
\end{INT}

\end{chapter6}
\begin{chapter7}
\section*{\S7: Martingales and Tight concentration}
\begin{itemise}
\item \textbf{The edge exposure martingale:} Let $H\sim G(n,p)$, and order the potential edges $e_1,\ldots,e_m$. Let $f$ be any graph-theoretic function. Define
\[X_i(H)=\Exp[f(G)\,|\,\textup{the first $i$ edges are as sampled, the rest considered random}]\]
Thus $X_0(H)=\Exp[f(H)]$, and $X_m(H)=f(H)$. The value at time $i$ is the expected value given you've seen the first $i$ actual outcomes.
\item \textbf{The vertex exposure martingale:} Similar, but this time, expose the first $i$ vertices and their internal edges. Note that this martingale has values $X_1,\ldots,X_n$, as exposing the first vertex does nothing.
\begin{INT}[Thm 7.2.1 [Azuma]:]
Let $0=X_0,\ldots,X_m$ be a martingale with $|X_{i+1}-X_i|\leq1$. Then for all $\lambda>0$:
\[\Prob[X_m>\lambda\sqrt m]<e^{-\lambda^2/2}.\]
\moreINT[Cor 7.2.2:]
Let $c=X_0,\ldots,X_m$ be a martingale with $|X_{i+1}-X_i|\leq1$. Then for all $\lambda>0$:
\[\Prob[|X_m-c|>\lambda\sqrt m]<2e^{-\lambda^2/2}.\]
\end{INT}
\item A graph-theoretic function $f$ is:
\begin{itemize}\squishlist
\item  \textbf{edge Lipschitz} if $|f(H)-f(H')|\leq1$ whenever $H$ and $H'$ differ by a single edge;
\item  \textbf{vertex Lipschitz} if $|f(H)-f(H')|\leq1$ whenever $H$ and $H'$ differ by addition and removal of vertices all connected to a given vertex.
\end{itemize}
\begin{INT}[Thm 7.2.3:]
If $f$ is edge (resp.\ vertex) Lipschitz, the corresponding edge (resp.\ vertex) exposure martingale has $|X_{i+1}-X_i|\leq1$.
\moreINT[Thm 7.2.4:]
Fix $n,p$ and let $c=\Exp[\chi(G)]$ where $G\sim G(n,p)$. Then
\[\Prob[|\chi(G)-c|>\lambda\sqrt{n-1}]<2e^{-\lambda^2/2}.\]
\begin{proof}
The vertex exposure martingale for $\chi(G)$, which has initial value $c$, is vertex-Lipschitz.
\end{proof}
This shows that the chromatic number is tightly concentrated around its mean.
\end{INT}
\item \textbf{Chromatic Number:} There's a proof here that:
\[\chi(G)\sim n/2\log_2n\textup{ almost surely, where $G\sim G(n,1/2)$}.\]
Let $\omega(G)$ be the clique number, and let $k=k_0-4$, where if $f(r)={n\choose r}2^{-{k\choose2}}$ is the average number of $K_r$'s in $G$, $k_0$ is the smallest value so that $f(k_0)<1$. Then $k\sim 2\log_2n$, and:
\begin{INT}[Thm 7.3.2:]
For some positive constant $c$, and $G\sim G(n,1/2)$:
\[\Prob[\omega(G)<k]<\exp\left({-(c+o(1))(n^2/(\log n)^8)}\right).\]
\end{INT}
Before proving this, we need a good martingale. Let $Y$ be the maximal size of a family of edge disjoint $k$-cliques!
\begin{INT}[Lem 7.3.1:]
$\Exp[Y]\geq(1+o(1))(n^2/2k^4)$. \textbf{(Understand the proof.)}
\begin{proof}[Proof of 7.3.2:]
Let $Y_0,\ldots,Y_m$, $m={n\choose2}$ be the edge exposure martingale. This is Lipschitz.
\begin{alignat*}{2}
 \qquad \qquad \qquad \qquad \Prob[\omega(G)<k]&=\Prob[Y=0]%&\qquad&\text{()}
\\&=\Prob[Y-\Exp[Y]\leq-\Exp[Y]]%&\qquad&\text{()}
\\&=\exp\left(-\frac{\Exp[Y]^2}{2{n\choose 2}}\right)&\qquad&\text{(Azuma)}
\\&<\exp\left({-(c+o(1))(n^2/(\log n)^8)}\right)&& \quad \qquad \qquad \qquad \qquad \qedhere%&\qquad&\text{()}
%\\&=%&\qquad&\text{()}
\end{alignat*}
\end{proof}
\end{INT}
\item \textbf{More on chromatic number:}
\begin{INT}[Thm 7.3.3:]
Let $p=n^{-\alpha}$ for fixed $\alpha>5/6$, and $G\sim G(n,p)$. Then there exists $u=u(n,p)$ so that almost always, $u\leq\chi(G)\leq u+3$, so that $\chi(G)$ is concentrated in four values.
\begin{proof}
Let $\epsilon>0$ be very small, and let $u$ be minimal such that $\Prob[\chi(G)\leq u]>\epsilon$. Let $Y$ be the minimal size of a set of vertices $S$ such that $G-S$ can be $u$-coloured. $Y$ is vertex-Lipschitz. Moreover, we have $\Prob[Y=0]>\epsilon$. Let $\mu=\Exp[Y]$, and let $\lambda$ be such that $e^{-\lambda^2/2}=\epsilon$. Then
\[\Prob[Y\leq\mu-\lambda\sqrt{n-1}]<\epsilon\implies\mu\leq\lambda\sqrt{n-1}.\]
But then,
\[\Prob[Y\geq2\lambda\sqrt{n-1}]\leq\Prob[Y\leq\mu+\lambda\sqrt{n-1}]\leq\epsilon.\]
Thus with probability at least $1-\epsilon$ there is a $u$-colouring of all but at most $c'\sqrt n$ vertices. By the following lemma, almost always (so at least with probability $1-\epsilon$), the remaining vertices can be 3-coloured, giving a $u+3$-colouring. As $u$ was minimal, there's at least a probability of $1-\epsilon$ that at least $u$ colours were needed. So there's a probability of $1-3\epsilon$ that the chromatic number is in this range.
\end{proof}
\moreINT[Lem 7.3.4:] For fixed $c$ and fixed $\alpha>5/6$, almost always every $c\sqrt n$ vertices of $G(n,n^{-\alpha})$ can be 3-coloured. \textbf{Read proof.}
\end{INT}

\item \textbf{A general setting:}
Given two sets $A,B$, let $[B,A]$ be the set of functions $B\to A$. Choose probabilities $p_{ab}$ in order to give a measure on {$[B,A]$}, via
\[\Prob[g(b)=a]=p_{ab}\]
Fix a gradation $\emptyset=B_0\subset\cdots\subset B_m=B$. Given a functional $L:[B,A]\to \R$, we define a martingale by the obvious:
\[X_i(h)=\Exp[L(g)\,|\,g(b)=h(b)\textup{ for all }b\in B_i].\]
There's an obvious notion of being Lipschitz relative to the gradation, in which case:
\begin{INT}[Thm 7.4.1:]
The corresponding martingale satisfies $|X_{i+1}-X_i|\leq1$.
\moreINT[Thm 7.4.2:]
If $\mu=\Exp[L(g)],$ then $\Prob[L(g)\geq\mu+\lambda\sqrt m]<e^{-\lambda^2/2}$ for all $\lambda>0$.
\end{INT}
\item \textbf{Another general setting:}
Suppose that the probability space is generated by a collection (indexed by $I$) of independent yes/no choices, with $p_i$ the probability that the $i^\textup{th}$ choice is yes.

\INDENT Let $Y$ be random variable, and let $c_i$ be the largest change to $Y$ that occurs if the $i^\textup{th}$ choice is reversed. Call $c_i$ the \textbf{effect} of $i$. Let $C$ be an upper bound on the $c_i$. Call $p_i(1-p_i)c_i^2$ the \textbf{variance} of choice $i$.

\INDENT  Suppose that Paul can ask an oracle the questions, one at a time, as he pleases. A \textbf{line of questioning} is then a list of questions determining the random variable $Y$ exactly. The \textbf{total variance} of a line of questioning is the sum of the variances of the queries in it.
\begin{INT}[Thm 7.4.3:]
For all $\epsilon>0$ there exists $\delta>0$ such that the following holds. Suppose that Paul has a strategy for finding $Y$ such that every line of questioning has total variance at most $\sigma^2$. Then
\[\Prob[|Y-\Exp[Y]|>\alpha\sigma]\leq2e^{-\alpha^2/(2(1+\epsilon))}\]
for all positive $\alpha$ with $\alpha C<\sigma(1+\epsilon)\delta$.
\end{INT}
\item \textbf{Illustration One:} Let $g$ be a random set endomorphism of $\{1,\ldots,n\}$, and let $L$ be the number of values not in the image of $g$. Then
\[\Exp[L]=n(1-n^{-1})^n\in\left(\frac{n-1}{e},\frac{n}{e}\right).\]
Let $B_i-\{1,\ldots,i\}$. Then $L$ is Lipschitz relative to this gradation, so that
\begin{INT}[Thm 7.5.1:]
$\Prob[|L-\frac{n}{e}|>\lambda\sqrt n+1]<2e^{-\lambda^2/2}$.
\end{INT}
\item \textbf{Illustration Two:} Let $B$ be a normed space, and $v_1,\ldots,v_n\in B$, each with $|B_i|\leq1$. Let $\epsilon_1,\ldots,\epsilon_n$ be independent $\pm$ random variables and let $X=|\sum\epsilon_iv_i|$. Then
\begin{INT}[Thm 7.5.2:]
$\Prob[X-\Exp[X]>\lambda\sqrt n]<e^{\lambda^2/2}$
\begin{proof}
Define a martingale $X_0,\ldots,X_n=X$ by exposing one of the $\epsilon_i$ at a time. Then
\begin{alignat*}{2}
|X_i(\epsilon)-X_{i+1}(\epsilon)|&=\left|(X_{i+1}(\epsilon)+X_{i+1}(\epsilon'))/2-X_{i+1}(\epsilon)\right|%&\qquad&\text{()}
\\&=\left|X_{i+1}(\epsilon')-X_{i+1}(\epsilon)\right|/2%&\qquad&\text{()}
\\&\leq1&\qquad&\text{(by theorem 7.4.1)}
%\\&=%&\qquad&\text{()}
%\\&=%&\qquad&\text{()}
\end{alignat*}
Here $\epsilon'$ differs from $\epsilon$ in the $(i+1)^\textup{st}$ position only. The result follows by Azuma's inequality.
\end{proof}
\end{INT}
\item \textbf{Illustration Three:} Consider the Hamming metric on $S=\{0,1\}^n$. For $A\subset S$, let $X$ be the distance from a random element of $S$ to the subset $A$. Suppose that $A$ has a density of $\epsilon$, and write $\epsilon=e^{-\lambda^2/2}$. Then
\begin{INT}[Thm 7.5.3:]
$\Prob[X\leq{2\lambda\sqrt n}]\geq(1-\epsilon)2^{n}$.
\begin{proof}
Let $X_0,\ldots,X_n=X$ be the martingale arising from uncovering one bit at a time.  This has the Lipschitz property. Write $\mu=\Exp[X]$, so that: 
\[\Prob[X<\mu-\lambda\sqrt n]<e^{-\lambda^2/2}=\epsilon\textup{\quad while\quad }\Prob[X\leq0]=\epsilon.\]
Thus $\mu<\lambda\sqrt n$, so that $\mu+\lambda\sqrt n<2\lambda\sqrt n$, and:
\[\epsilon>\Prob[X>\mu+\lambda\sqrt n]\geq\Prob[X>2\lambda\sqrt n].\qedhere\]
\end{proof}
\end{INT}
\item \textbf{Illustration Four} I did not read.
\end{itemise}
\subsection*{Talagrand's inequality}
Let $\Omega=\prod_{i=1}^n\Omega_i$ be a finite product of probability spaces. For $A\subset\Omega$ and $x\in \Omega$, Talagrand defines a notion of ``the distance from $x$ to $A$'', $\rho(A,x)$, as follows.

\INDENT Suppose that our adversary $\scrD$ can set the cost of changing the $i^\textup{th}$ coordinate of $x$ at a quantity $\alpha_i\in\R$. $\scrD$ is trying to maximise the cost of returning to $A$ from $x$. $\scrD$'s only constraint is that $|(\alpha_1,\ldots,\alpha_n)|\leq1$.
\begin{INT}[Thm 7.6.1 {[Talagrand's Inequality]}:] For $t\geq0$, let $A_t$ be the event that a randomly chosen element of $\Omega$ is at most a distance of $t$ from $A$. Then
\[\Prob[A]\Prob[\overline{A_t}]\leq e^{-t^2/4}.\]
In particular, if $\Prob[A]>c$ for some fixed $c$, then as $t$ increases, a random point will very likely be within a distance $t$ of $A$.
\end{INT}
Note that when $\Omega=\{0,1\}^n$, $\rho(A,x)$ is the Euclidean distance from $x$ to the convex hull of $A$.
\subsection*{Applications of Talagrand's inequality}
Call a function $h:\Omega\to\R$ \textbf{$K$-Lipschitz} if $|h(x)-h(y)|\leq K$ when $x$ and $y$ differ in only one coordinate. Given a function $f:\N\to\N$, say that $h$ is \textbf{$f$-certifiable} if whenever $h(x)\geq s$ there is a collection $I$ of at most $f(s)$ indices such that $h(y)\geq s$ for all $y$ agreeing with $x$ on $I$.
\end{chapter7}
\begin{INT}[Thm 7.7.1:]
Suppose that $h$ is $K$-Lipschitz, and $f$-certifiable. Then for all $b,t$:
\[\Prob[X\leq b-tK\sqrt{f(b)}]\Prob[X\geq b]\leq e^{-t^2/4}.\]
Note that to be $f$-certifiable means that ``you can tell that $h$ will take a large value (over $s$) only by looking at $f(s)$ of the probability spaces''. For example, $G(n,p)$ is the product of ${n\choose2}$ Bernoulli random variables. Let $h$ be the number of triangles in the random graph. $h$ is $f$-certifiable, where $f(s)=3s$.
\end{INT}
We normally let $b$ be the median, and so produce concentration results around the median with Talagrand's inequality. Azuma's inequality produces concentration results around the mean.
\begin{itemise}
\item Consider the \textbf{longest increasing subsequence} in $(x_1,\ldots,x_n)$, with $x_i\sim U(0,1)$. Let $X$ be its length. Then $X$ is Lipschitz, and $f$-certifiable, with $f(s)=s$ the identity. Let $b=m$ be the median of $X$. Then
\[\Prob[X<m-t\sqrt m]\leq e^{-t^2/4}/\Prob[X\geq m]\leq 2e^{-t^2/4}.\]
By elementary methods, $m=\Theta(\sqrt n)$, so that for $s\gg n^{1/4}$ we have $X>m-s$ almost surely. For the opposite bound, let $b$ be such that $b-t\sqrt b=m$. Then $b=m+(1+o(1))t\sqrt m$, and:
\[\Prob[X\geq b]\leq e^{-t^2/4}/\Prob[X\leq m]\leq 2e^{-t^2/4}.\]
Again, letting $t\rightarrow\infty$ slowly, $X<m+\sqrt t\sqrt m=m+s$ almost surely, where $s=\sqrt t\sqrt m \gg n^{1/4}$. Thus $|X-m|<s$ almost surely whenever $s\gg n^{1/4}$.
\item  Let $\omega(G)$ be the clique number, and let $k=k_0-4$, where if $f(r)={n\choose r}2^{-{r\choose2}}$ is the average number of $K_r$'s in $G(n,1/2)$, $k_0$ is the smallest value so that $f(k_0)<1$. Then $k\sim 2\log_2n$. Let $Y$ be the maximal size of a family of edge disjoint $k$-cliques. We skipped the proof of the following.
\begin{INT}[Thm 7.3.2:]
For some positive constant $c$, and $G\sim G(n,1/2)$:
\[\Prob[\omega(G)<k]<e^{-(c+o(1))(n^2/(\log n)^8)}.\]
``There's some number $k_0$ where you have on average less than one $K_{K_0}$. If $k$ is four less, you are very likely to have a $K_k$.''
\end{INT}
We also saw that $m(Y)=\Omega(n^2k^{-4})$, as this is the mean, and we had a tight concentration result. Now $Y$ is Lipschitz, and $f$-certifiable, where $f(s)={n\choose2}s$. Then
\begin{alignat*}{2}
\Prob[\omega(G)<k]&=\Prob[Y\leq 0]%&\qquad&\text{()}
\\&=\Prob\left[Y\leq m-t\sqrt m{k\choose 2}^{1/2}\right]&\qquad&\text{setting $t=\sqrt {\frac{m}{{k\choose2}}}=\Omega\left(\frac{n}{(2\log_2n)^3}\right)$}
\\&<e^{-t^2/4}/\Prob[Y\geq m]%&\qquad&\text{()}
\\&<2\exp\left(-\Omega\left(\frac{n^2}{2^8(\log_2n)^6}\right)\right)%&\qquad&\text{()}
\\&=\exp\left(-\Omega(n^2/(\log_2n)^6)\right)&\qquad&\text{(an improvement on 7.3.2).}
\end{alignat*}
\end{itemise}


\end{document}













